# Qwen3-0.6B è‰ç¨¿æ¨¡å‹å®Œæ•´å®ç°

åŸºäº Qwen3-0.6B æ„å»ºçš„è‰ç¨¿æ¨¡å‹ï¼ŒåŒ…å«å‡åŒ€é‡‡æ ·å±‚å’ŒçŸ¥è¯†å¢å¼ºåŠŸèƒ½ã€‚å·²è®­ç»ƒå®Œæˆï¼Œé€Ÿåº¦æ¯”åŸæ¨¡å‹å¿« **1.64å€**ã€‚

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
CrossAndAttention/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ qwen3_0.6b_config.yaml          # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_loader.py                   # æ¨¡å‹åŠ è½½å™¨
â”‚   â”œâ”€â”€ layer_sampler.py                 # å±‚é‡‡æ ·ç­–ç•¥
â”‚   â”œâ”€â”€ knowledge_cache.py               # çŸ¥è¯†ç¼“å­˜ç®¡ç†
â”‚   â””â”€â”€ knowledge_enhanced_draft.py      # çŸ¥è¯†å¢å¼ºè‰ç¨¿æ¨¡å‹
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ draft_trainer.py                 # è®­ç»ƒå™¨
â”‚   â””â”€â”€ data_utils.py                    # æ•°æ®å¤„ç†å·¥å…·
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ speculative_decoder.py           # æ¨æµ‹è§£ç å™¨
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_draft.py                   # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ build_knowledge_cache.py         # æ„å»ºçŸ¥è¯†ç¼“å­˜
â”‚   â”œâ”€â”€ test_trained_model.py            # æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
â”‚   â””â”€â”€ benchmark_speed.py                # é€Ÿåº¦å¯¹æ¯”æµ‹è¯•
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â””â”€â”€ best_draft_model_epoch5.pth  # æœ€ä½³æ¨¡å‹ï¼ˆ4.3GBï¼‰
â”‚   â””â”€â”€ knowledge_cache/
â”‚       â””â”€â”€ knowledge_cache.pth          # çŸ¥è¯†ç¼“å­˜
â”œâ”€â”€ requirements.txt                     # ä¾èµ–åŒ…
â””â”€â”€ README.md                            # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè®¾ç½®

```bash
pip install -r requirements.txt
```

### 2. æ„å»ºçŸ¥è¯†ç¼“å­˜ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰

```bash
python scripts/build_knowledge_cache.py
```

è¿™å°†ï¼š
- åŠ è½½ Qwen3-0.6B åŸºç¡€æ¨¡å‹
- å¯¹å¸¸è§é—®é¢˜è¿›è¡Œå‰å‘ä¼ æ’­
- æå–å¹¶å­˜å‚¨ KV ç¼“å­˜
- ä¿å­˜åˆ° `output/knowledge_cache/knowledge_cache.pth`

### 3. è®­ç»ƒè‰ç¨¿æ¨¡å‹

```bash
python scripts/train_draft.py
```

è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ï¼š
- çŸ¥è¯†è’¸é¦ï¼ˆKLæ•£åº¦æŸå¤±ï¼‰
- äº¤å‰ç†µæŸå¤±
- éªŒè¯å’Œæ£€æŸ¥ç‚¹ä¿å­˜
- è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹

**è®­ç»ƒç»“æœ**ï¼š
- è®­ç»ƒæŸå¤±: 8.85
- éªŒè¯æŸå¤±: 7.95
- è®­ç»ƒè½®æ•°: 5 epochs
- é‡‡æ ·å±‚: [0, 5, 11, 16, 22, 27] (ä»28å±‚ä¸­é‡‡æ ·6å±‚)

### 4. æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹

```bash
python scripts/test_trained_model.py
```

### 5. é€Ÿåº¦å¯¹æ¯”æµ‹è¯•

```bash
python scripts/benchmark_speed.py
```

**æµ‹è¯•ç»“æœ**ï¼š
- è‰ç¨¿æ¨¡å‹é€Ÿåº¦: **11.29 tokens/s**
- ç›®æ ‡æ¨¡å‹é€Ÿåº¦: 8.85 tokens/s
- **åŠ é€Ÿæ¯”: 1.64x** (å¿«63.6%)

## ğŸ“‹ ä¸»è¦ç‰¹æ€§

### 1. å‡åŒ€å±‚é‡‡æ ·
- ä» Qwen3-0.6B çš„28å±‚ä¸­æ™ºèƒ½é‡‡æ ·6å±‚
- æ”¯æŒå‡åŒ€ã€å‡ ä½•ã€å¯¹æ•°ä¸‰ç§é‡‡æ ·ç­–ç•¥
- è‡ªåŠ¨åŒ…å«é¦–å±‚å’Œå°¾å±‚

### 2. çŸ¥è¯†å¢å¼º
- äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
- çŸ¥è¯†ç¼“å­˜æŸ¥è¯¢ï¼ˆ15ä¸ªå¸¸è§é—®é¢˜çš„KVç¼“å­˜ï¼‰
- é—¨æ§èåˆæœºåˆ¶

### 3. å®Œæ•´è®­ç»ƒæµç¨‹
- çŸ¥è¯†è’¸é¦è®­ç»ƒ
- æ”¯æŒæ¢¯åº¦ç´¯ç§¯
- å­¦ä¹ ç‡è°ƒåº¦
- è‡ªåŠ¨æ£€æŸ¥ç‚¹ä¿å­˜
- æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
- æ¢¯åº¦è£å‰ª

### 4. æ¨æµ‹è§£ç 
- ä½¿ç”¨è‰ç¨¿æ¨¡å‹åŠ é€Ÿæ¨ç†
- ç›®æ ‡æ¨¡å‹éªŒè¯æœºåˆ¶
- è‡ªåŠ¨æ¥å—/æ‹’ç»ç­–ç•¥

## âš™ï¸ é…ç½®è¯´æ˜

ä¸»è¦é…ç½®æ–‡ä»¶ï¼š`configs/qwen3_0.6b_config.yaml`

### è‰ç¨¿æ¨¡å‹é…ç½®
```yaml
draft_model:
  sampling_strategy: "uniform"  # uniform, geometric, logarithmic
  num_sampled_layers: 6
  sampled_indices: [0, 5, 11, 16, 22, 27]
  add_knowledge_enhancement: true
```

### è®­ç»ƒé…ç½®
```yaml
training:
  batch_size: 8
  learning_rate: 3e-5
  num_epochs: 5
  use_knowledge_distillation: true
  kl_divergence_weight: 0.8
  max_seq_length: 512
  gradient_accumulation_steps: 2
```

### çŸ¥è¯†å¢å¼ºé…ç½®
```yaml
knowledge_enhancement:
  enabled: true
  cache_dim: 512
  num_heads: 8
  fusion_gate: true
```

## ğŸ“Š è®­ç»ƒç»“æœ

### æ€§èƒ½æŒ‡æ ‡
- **è®­ç»ƒæŸå¤±**: 8.85 (ä»åˆå§‹ 502.78 ä¸‹é™)
- **éªŒè¯æŸå¤±**: 7.95 (æœ€ä½³æ¨¡å‹)
- **éªŒè¯å›°æƒ‘åº¦**: 2833.54
- **è®­ç»ƒè½®æ•°**: 5 epochs
- **é‡‡æ ·å±‚**: [0, 5, 11, 16, 22, 27]

### é€Ÿåº¦å¯¹æ¯”
- **è‰ç¨¿æ¨¡å‹**: 11.29 tokens/s
- **ç›®æ ‡æ¨¡å‹**: 8.85 tokens/s
- **åŠ é€Ÿæ¯”**: 1.64x
- **é€Ÿåº¦æå‡**: 63.6%

### æ¨¡å‹æ–‡ä»¶
- **æœ€ä½³æ¨¡å‹**: `output/checkpoints/best_draft_model_epoch5.pth` (4.3 GB)
- **çŸ¥è¯†ç¼“å­˜**: `output/knowledge_cache/knowledge_cache.pth`

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### åŠ è½½å¹¶æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹

```python
import torch
import yaml
from models.base_loader import Qwen3Loader
from models.knowledge_enhanced_draft import Qwen3DraftModel

# åŠ è½½é…ç½®
with open("configs/qwen3_0.6b_config.yaml", 'r') as f:
    config = yaml.safe_load(f)

# åŠ è½½åŸºç¡€æ¨¡å‹å’Œtokenizer
loader = Qwen3Loader("configs/qwen3_0.6b_config.yaml")
target_model = loader.load_target_model(device='cpu')
tokenizer = loader.load_tokenizer()

# ç¡®ä¿pad_tokenè®¾ç½®
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½çŸ¥è¯†ç¼“å­˜
from models.knowledge_cache import KnowledgeCacheManager
cache_path = "output/knowledge_cache/knowledge_cache.pth"
cache_data = torch.load(cache_path, map_location='cpu')
knowledge_cache_manager = KnowledgeCacheManager(
    hidden_size=config['base_model']['hidden_size'],
    num_heads=config['base_model']['num_attention_heads'],
    cache_dim=config['knowledge_enhancement']['cache_dim']
)
knowledge_cache_manager.kv_cache = cache_data.get('kv_cache', {})

# åˆ›å»ºå¹¶åŠ è½½è‰ç¨¿æ¨¡å‹
draft_model = Qwen3DraftModel(config, target_model, knowledge_cache_manager=knowledge_cache_manager)
draft_model = draft_model.cpu()
draft_model.eval()

# åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
checkpoint = torch.load(
    "output/checkpoints/best_draft_model_epoch5.pth",
    map_location='cpu'
)
draft_model.load_state_dict(checkpoint['model_state_dict'])

# æ¨ç†
text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œ"
inputs = tokenizer(text, return_tensors='pt', padding=True)

# ç¡®ä¿attention_maskå­˜åœ¨
if 'attention_mask' not in inputs:
    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id
    inputs['attention_mask'] = (inputs['input_ids'] != pad_token_id).long()

with torch.no_grad():
    outputs = draft_model(**inputs)
    logits = outputs['logits']
    
    # è·å–ä¸‹ä¸€ä¸ªtoken
    next_token_logits = logits[:, -1, :]
    next_token = torch.argmax(next_token_logits, dim=-1)
    predicted = tokenizer.decode([next_token.item()])
    
    print(f"è¾“å…¥: {text}")
    print(f"é¢„æµ‹: {predicted}")
```

### ä½¿ç”¨æ¨æµ‹è§£ç 

```python
from inference.speculative_decoder import SpeculativeDecoder

decoder = SpeculativeDecoder(draft_model, target_model, tokenizer, gamma=4)
generated = decoder.generate(
    input_ids,
    max_new_tokens=50,
    temperature=0.8,
    top_p=0.9
)
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **æ¨¡å‹ä¸‹è½½**: é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½ Qwen3-0.6B æ¨¡å‹ï¼Œç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸
2. **è®¾å¤‡æ”¯æŒ**: æ”¯æŒ CUDAã€MPS (Apple Silicon) å’Œ CPU
3. **è®­ç»ƒæ—¶é—´**: å®Œæ•´è®­ç»ƒå¯èƒ½éœ€è¦æ•°å°æ—¶ï¼Œå–å†³äºç¡¬ä»¶é…ç½®
4. **æ•°æ®å‡†å¤‡**: å½“å‰ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼Œå®é™…åº”ç”¨éœ€è¦å‡†å¤‡çœŸå®è®­ç»ƒæ•°æ®
5. **Attention Mask**: å·²è‡ªåŠ¨å¤„ç†ï¼Œä¸å†å‡ºç° pad_token == eos_token çš„è­¦å‘Š
6. **æ•°å€¼ç¨³å®šæ€§**: å·²æ·»åŠ  NaN/Inf æ£€æµ‹å’Œæ¢¯åº¦è£å‰ª

## ğŸ” å·²ä¿®å¤çš„é—®é¢˜

1. âœ… **Attention Mask è­¦å‘Š**: å·²è‡ªåŠ¨åˆ›å»º attention_maskï¼Œä¸å†å‡ºç°è­¦å‘Š
2. âœ… **æ•°å€¼ç¨³å®šæ€§**: æ·»åŠ äº† NaN/Inf æ£€æµ‹å’Œæ¢¯åº¦è£å‰ª
3. âœ… **æ¨¡å‹ä¿å­˜**: è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ£€æŸ¥ç‚¹
4. âœ… **è¿›åº¦æ˜¾ç¤º**: è®­ç»ƒè¿‡ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨çœŸå®æ•°æ®**: ä½¿ç”¨çœŸå®æ•°æ®é›†å¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹è´¨é‡
2. **è°ƒæ•´è¶…å‚æ•°**: å¯ä»¥å°è¯•ä¸åŒçš„å­¦ä¹ ç‡ã€batch sizeç­‰
3. **ç»§ç»­è®­ç»ƒ**: å¯ä»¥ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒæ›´å¤šè½®æ¬¡
4. **é‡åŒ–åŠ é€Ÿ**: è€ƒè™‘ä½¿ç”¨æ¨¡å‹é‡åŒ–è¿›ä¸€æ­¥åŠ é€Ÿ

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªåŸ Qwen æ¨¡å‹çš„è®¸å¯è¯ã€‚
