# å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“ æ¨¡å‹æ¶æ„

### è‰ç¨¿æ¨¡å‹ç»“æ„

è‰ç¨¿æ¨¡å‹åŸºäº Qwen3-0.6Bï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼æ„å»ºï¼š

1. **å±‚é‡‡æ ·**: ä»åŸºç¡€æ¨¡å‹çš„28å±‚ä¸­å‡åŒ€é‡‡æ ·6å±‚ `[0, 5, 11, 16, 22, 27]`
2. **çŸ¥è¯†å¢å¼º**: åœ¨æ¯ä¸ªé‡‡æ ·å±‚åæ·»åŠ äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨é¢„ç¼“å­˜çš„KVçŸ©é˜µ
3. **è¿‡æ¸¡å±‚**: åœ¨é‡‡æ ·å±‚ä¹‹é—´æ·»åŠ è¿‡æ¸¡å±‚ï¼Œå¼¥è¡¥è¯­ä¹‰æ–­å±‚

### æ¶æ„æµç¨‹

```
è¾“å…¥æ–‡æœ¬
  â†“
Token Embedding
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é‡‡æ ·å±‚ 0: Self-Attention + MLP      â”‚
â”‚  â†“                                  â”‚
â”‚  Cross-Attention (çŸ¥è¯†å¢å¼º)         â”‚
â”‚  â†“                                  â”‚
â”‚  è¿‡æ¸¡å±‚                              â”‚
â”‚  â†“                                  â”‚
â”‚ é‡‡æ ·å±‚ 5: Self-Attention + MLP      â”‚
â”‚  â†“                                  â”‚
â”‚  Cross-Attention (çŸ¥è¯†å¢å¼º)         â”‚
â”‚  â†“                                  â”‚
â”‚  è¿‡æ¸¡å±‚                              â”‚
â”‚  ... (é‡å¤åˆ°é‡‡æ ·å±‚ 27)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Layer Norm
  â†“
LM Head
  â†“
è¾“å‡º Logits
```

### çŸ¥è¯†å¢å¼ºæœºåˆ¶

- **çŸ¥è¯†ç¼“å­˜**: é¢„è®¡ç®—å¸¸è§é—®é¢˜çš„ç­”æ¡ˆéƒ¨åˆ†KVçŸ©é˜µ
- **äº¤å‰æ³¨æ„åŠ›**: ä½¿ç”¨å½“å‰hidden statesä½œä¸ºqueryï¼ŒçŸ¥è¯†KVä½œä¸ºkey/value
- **é—¨æ§èåˆ**: åŠ¨æ€èåˆåŸå§‹ç‰¹å¾å’ŒçŸ¥è¯†å¢å¼ºç‰¹å¾

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
pip install -r requirements.txt
```

### 2. æ„å»ºçŸ¥è¯†ç¼“å­˜ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰

```bash
python scripts/build_knowledge_cache.py
```

è¿™å°†ï¼š
- åŠ è½½ Qwen3-0.6B åŸºç¡€æ¨¡å‹
- å¯¹15ä¸ªå¸¸è§é—®é¢˜ç”Ÿæˆç­”æ¡ˆå¹¶æå–KVçŸ©é˜µ
- ä¿å­˜åˆ° `output/knowledge_cache/knowledge_cache.pth`

### 3. è®­ç»ƒæ¨¡å‹

```bash
python scripts/train_with_knowledge.py
```

è®­ç»ƒé…ç½®ï¼ˆåœ¨ `configs/qwen3_0.6b_config.yaml` ä¸­ï¼‰ï¼š
- **è®­ç»ƒè½®æ•°**: 5 epochs
- **æ‰¹æ¬¡å¤§å°**: 8
- **å­¦ä¹ ç‡**: 3e-5
- **çŸ¥è¯†è’¸é¦**: å¯ç”¨ï¼ˆKLæ•£åº¦æƒé‡ 0.8ï¼‰
- **æ¥å—æ¦‚ç‡æŸå¤±**: å¯ç”¨ï¼ˆæƒé‡ 0.3ï¼‰

è®­ç»ƒå®Œæˆåï¼Œæœ€ä½³æ¨¡å‹ä¿å­˜åœ¨ï¼š
- `output/checkpoints/best_draft_model_knowledge_epoch5.pth`

### 4. æ¨ç†æµ‹è¯•

```bash
python scripts/test_knowledge_questions.py
```

æµ‹è¯•çŸ¥è¯†åº“ä¸­çš„é—®é¢˜ï¼ŒæŸ¥çœ‹æ¥å—ç‡å’Œç”Ÿæˆè´¨é‡ã€‚

### 5. æ¨æµ‹è§£ç åŸºå‡†æµ‹è¯•

```bash
python scripts/benchmark_speculative_decoding.py
```

å¯¹æ¯”ç›´æ¥ç”Ÿæˆå’Œæ¨æµ‹è§£ç çš„æ€§èƒ½ã€‚

---

## ğŸ’» ä»£ç ç¤ºä¾‹

### åŠ è½½æ¨¡å‹å¹¶æ¨ç†

```python
import torch
import yaml
from models.base_loader import Qwen3Loader
from models.knowledge_enhanced_draft import Qwen3DraftModel
from models.knowledge_cache import KnowledgeCacheManager

# 1. åŠ è½½é…ç½®
with open("configs/qwen3_0.6b_config.yaml", 'r') as f:
    config = yaml.safe_load(f)

# 2. åŠ è½½åŸºç¡€æ¨¡å‹
loader = Qwen3Loader("configs/qwen3_0.6b_config.yaml")
target_model = loader.load_target_model(device='cpu')
tokenizer = loader.load_tokenizer()

# 3. åŠ è½½çŸ¥è¯†ç¼“å­˜
cache_path = "output/knowledge_cache/knowledge_cache.pth"
cache_data = torch.load(cache_path, map_location='cpu')
knowledge_cache_manager = KnowledgeCacheManager(
    hidden_size=config['base_model']['hidden_size'],
    num_heads=config['base_model']['num_attention_heads'],
    cache_dim=config['knowledge_enhancement']['cache_dim']
)
knowledge_cache_manager.kv_cache = cache_data.get('kv_cache', {})

# 4. åˆ›å»ºè‰ç¨¿æ¨¡å‹
draft_model = Qwen3DraftModel(config, target_model, knowledge_cache_manager)
draft_model.eval()

# 5. åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
checkpoint = torch.load(
    "output/checkpoints/best_draft_model_knowledge_epoch5.pth",
    map_location='cpu'
)
draft_model.load_state_dict(checkpoint['model_state_dict'])

# 6. æ¨ç†
text = "æ·±åº¦å­¦ä¹ æ˜¯"
inputs = tokenizer(text, return_tensors='pt')
with torch.no_grad():
    outputs = draft_model(inputs['input_ids'], retrieve_knowledge=True, query_text=text)
    logits = outputs['logits']
    next_token = torch.argmax(logits[:, -1, :], dim=-1)
    print(f"è¾“å…¥: {text}")
    print(f"é¢„æµ‹: {tokenizer.decode([next_token.item()])}")
```

### ä½¿ç”¨æ¨æµ‹è§£ç 

```python
from inference.speculative_decoder import SpeculativeDecoder

# åˆ›å»ºæ¨æµ‹è§£ç å™¨ï¼ˆgamma=5è¡¨ç¤ºè‰ç¨¿æ¨¡å‹ä¸€æ¬¡ç”Ÿæˆ5ä¸ªtokenï¼‰
decoder = SpeculativeDecoder(draft_model, target_model, tokenizer, gamma=5)

# ç”Ÿæˆæ–‡æœ¬
input_ids = tokenizer("æ·±åº¦å­¦ä¹ æ˜¯", return_tensors='pt')['input_ids']
generated = decoder.generate(
    input_ids=input_ids,
    max_new_tokens=20,
    temperature=1.0,
    top_p=0.9
)

print(tokenizer.decode(generated[0]))
```

---

## ğŸ“Š æ¨¡å‹æ€§èƒ½

- **å‚æ•°é‡**: 686Mï¼ˆæ¯”åŸºç¡€æ¨¡å‹å¤š15%ï¼Œå› ä¸ºæ·»åŠ äº†çŸ¥è¯†å¢å¼ºå±‚ï¼‰
- **æ¨ç†é€Ÿåº¦**: æ¯”åŸºç¡€æ¨¡å‹å¿«çº¦1.6å€ï¼ˆå› ä¸ºåªæœ‰6å±‚ï¼‰
- **æ¥å—ç‡**: 32-80%ï¼ˆå–å†³äºé—®é¢˜ç±»å‹ï¼‰

---

## âš™ï¸ é…ç½®è¯´æ˜

ä¸»è¦é…ç½®æ–‡ä»¶ï¼š`configs/qwen3_0.6b_config.yaml`

### å…³é”®é…ç½®é¡¹

```yaml
draft_model:
  num_sampled_layers: 6  # é‡‡æ ·å±‚æ•°
  sampling_strategy: "uniform"  # é‡‡æ ·ç­–ç•¥

knowledge_enhancement:
  enabled: true  # å¯ç”¨çŸ¥è¯†å¢å¼º
  cache_dim: 512  # ç¼“å­˜ç»´åº¦

training:
  num_epochs: 5
  batch_size: 8
  learning_rate: 3e-5
  kl_divergence_weight: 0.8  # çŸ¥è¯†è’¸é¦æƒé‡
  acceptance_loss_weight: 0.3  # æ¥å—æ¦‚ç‡æŸå¤±æƒé‡
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **é¦–æ¬¡è¿è¡Œ**: éœ€è¦ä¸‹è½½ Qwen3-0.6B æ¨¡å‹ï¼ˆçº¦1.2GBï¼‰
2. **è®­ç»ƒæ—¶é—´**: CPUè®­ç»ƒçº¦éœ€1-2å°æ—¶ï¼ŒGPU/MPSæ›´å¿«
3. **å†…å­˜éœ€æ±‚**: è®­ç»ƒæ—¶çº¦éœ€8-10GBå†…å­˜
4. **çŸ¥è¯†ç¼“å­˜**: åŒ…å«15ä¸ªå¸¸è§é—®é¢˜çš„ç­”æ¡ˆéƒ¨åˆ†KVçŸ©é˜µ

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- **é…ç½®æ–‡ä»¶**: `configs/qwen3_0.6b_config.yaml`
- **è®­ç»ƒè„šæœ¬**: `scripts/train_with_knowledge.py`
- **æµ‹è¯•è„šæœ¬**: `scripts/test_knowledge_questions.py`
- **çŸ¥è¯†ç¼“å­˜æ„å»º**: `scripts/build_knowledge_cache.py`
- **æ¨æµ‹è§£ç å™¨**: `inference/speculative_decoder.py`

