#!/usr/bin/env python3
"""
æ¯”è¾ƒè‰ç¨¿æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´å¥å­ç›¸ä¼¼åº¦
"""

import os
import sys
import torch
import yaml
from pathlib import Path
from typing import List, Dict
import numpy as np

sys.path.insert(0, str(Path(__file__).parent.parent))

from models.base_loader import Qwen3Loader
from models.knowledge_enhanced_draft import Qwen3DraftModel
from models.knowledge_cache import KnowledgeCacheManager

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("è­¦å‘Š: sentence-transformers æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦")

def load_models(config_path: str, checkpoint_path: str = None):
    """åŠ è½½æ¨¡å‹"""
    print("="*70)
    print("åŠ è½½æ¨¡å‹")
    print("="*70)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("\n1. åŠ è½½åŸºç¡€æ¨¡å‹...")
    loader = Qwen3Loader(config_path)
    target_model = loader.load_target_model(device='cpu')
    tokenizer = loader.load_tokenizer()
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½çŸ¥è¯†ç¼“å­˜
    print("\n2. åŠ è½½çŸ¥è¯†ç¼“å­˜...")
    cache_path = "output/knowledge_cache/knowledge_cache.pth"
    knowledge_cache_manager = None
    
    if os.path.exists(cache_path):
        cache_data = torch.load(cache_path, map_location='cpu', weights_only=False)
        
        knowledge_config = config.get('knowledge_enhancement', {})
        use_vector_retrieval = knowledge_config.get('use_vector_retrieval', True)
        embedding_model_name = knowledge_config.get('embedding_model_name', None)
        
        knowledge_cache_manager = KnowledgeCacheManager(
            hidden_size=config['base_model']['hidden_size'],
            num_heads=config['base_model']['num_attention_heads'],
            cache_dim=config['knowledge_enhancement']['cache_dim'],
            use_vector_retrieval=use_vector_retrieval,
            embedding_model_name=embedding_model_name,
            target_model=target_model,  # ä½¿ç”¨ç›®æ ‡æ¨¡å‹çš„åµŒå…¥å±‚
            tokenizer=tokenizer  # ä¼ å…¥tokenizer
        )
        knowledge_cache_manager.kv_cache = cache_data.get('kv_cache', {})
        
        if 'knowledge_embeddings' in cache_data:
            knowledge_cache_manager.knowledge_embeddings = cache_data['knowledge_embeddings']
        
        print(f"âœ“ çŸ¥è¯†ç¼“å­˜åŠ è½½å®Œæˆï¼Œå…± {len(knowledge_cache_manager.kv_cache)} ä¸ªçŸ¥è¯†é¡¹")
    
    # åˆ›å»ºè‰ç¨¿æ¨¡å‹
    print("\n3. åˆ›å»ºè‰ç¨¿æ¨¡å‹...")
    draft_model = Qwen3DraftModel(config, target_model, knowledge_cache_manager=knowledge_cache_manager)
    draft_model = draft_model.cpu()
    draft_model.eval()
    target_model.eval()
    
    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡ï¼ˆä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„best checkpointï¼‰
    if checkpoint_path is None:
        checkpoint_dir = "output/checkpoints"
        # ä¼˜å…ˆæŸ¥æ‰¾best checkpointï¼ŒæŒ‰epochæ’åº
        knowledge_checkpoints = [f for f in os.listdir(checkpoint_dir) 
                                if 'knowledge' in f and 'best' in f and f.endswith('.pth')]
        if knowledge_checkpoints:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
            knowledge_checkpoints.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)
            checkpoint_path = os.path.join(checkpoint_dir, knowledge_checkpoints[0])
            print(f"  è‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„checkpoint: {knowledge_checkpoints[0]}")
    
    if checkpoint_path and os.path.exists(checkpoint_path):
        print(f"\n4. åŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        # ä½¿ç”¨strict=Falseä»¥å…¼å®¹æ¨¡å‹ç»“æ„å˜åŒ–ï¼ˆå¦‚æ–°å¢çš„å½’ä¸€åŒ–å±‚ï¼‰
        missing_keys, unexpected_keys = draft_model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        print(f"âœ“ æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
        print(f"  - Epoch: {checkpoint.get('epoch', 'N/A')}")
        print(f"  - éªŒè¯æŸå¤±: {checkpoint.get('val_loss', 0):.4f}")
        if missing_keys:
            print(f"  âš  ç¼ºå¤±çš„é”®: {len(missing_keys)}ä¸ªï¼ˆå¯èƒ½æ˜¯æ–°å¢çš„å±‚ï¼‰")
        if unexpected_keys:
            print(f"  âš  å¤šä½™çš„é”®: {len(unexpected_keys)}ä¸ª")
    else:
        print(f"\nâš  æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨æœªè®­ç»ƒçš„æ¨¡å‹")
    
    return draft_model, target_model, tokenizer, knowledge_cache_manager

def generate_full_sentence(model, tokenizer, prompt: str, max_new_tokens: int = 50, 
                          knowledge_cache_manager=None, query_text=None, temperature: float = 0.7):
    """ç”Ÿæˆå®Œæ•´å¥å­
    
    Args:
        temperature: æ¸©åº¦ç¼©æ”¾å‚æ•°ï¼Œ<1ä½¿åˆ†å¸ƒæ›´å°–é”ï¼ˆæé«˜ç¡®å®šæ€§ï¼‰ï¼Œ>1ä½¿åˆ†å¸ƒæ›´å¹³æ»‘ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
    """
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs['input_ids']
    
    # å¦‚æœæ˜¯è‰ç¨¿æ¨¡å‹ï¼Œéœ€è¦ä¼ é€’çŸ¥è¯†ç¼“å­˜
    if isinstance(model, Qwen3DraftModel):
        # æ£€ç´¢çŸ¥è¯†
        knowledge_cache = None
        if knowledge_cache_manager is not None and query_text is not None:
            knowledge_config = model.config.get('knowledge_enhancement', {})
            threshold = knowledge_config.get('retrieval_threshold', 0.7)
            retrieved = knowledge_cache_manager.retrieve(query_text, threshold=threshold)
            if retrieved is not None:
                knowledge_cache = retrieved
        
        # ç”Ÿæˆtokenï¼ˆæ·»åŠ é‡å¤æƒ©ç½šï¼Œç‰¹åˆ«å¤„ç†æ¢è¡Œç¬¦ï¼‰
        current_input = input_ids
        generated = []
        eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id
        
        # é‡å¤æƒ©ç½šå‚æ•°
        repetition_penalty = 1.2  # å¯¹å·²ç”Ÿæˆçš„tokené™ä½æ¦‚ç‡ï¼ˆæé«˜æƒ©ç½šï¼‰
        no_repeat_ngram_size = 2  # ç¦æ­¢2-gramé‡å¤
        newline_penalty = 3.0  # å¯¹æ¢è¡Œç¬¦çš„é¢å¤–æƒ©ç½šï¼ˆå¢å¼ºï¼‰
        
        # è¯†åˆ«æ¢è¡Œç¬¦token ID
        newline_token_ids = set()
        for test_str in ['\n', '\r\n', '\r']:
            try:
                token_ids = tokenizer.encode(test_str, add_special_tokens=False)
                newline_token_ids.update(token_ids)
            except:
                pass
        
        # ç”¨äºè·Ÿè¸ªå·²ç”Ÿæˆçš„tokenï¼ˆç”¨äºé‡å¤æƒ©ç½šï¼‰
        generated_token_ids = []
        consecutive_newlines = 0  # è¿ç»­æ¢è¡Œç¬¦è®¡æ•°
        
        for i in range(max_new_tokens):
            outputs = model.forward(
                current_input, 
                knowledge_cache=knowledge_cache,
                retrieve_knowledge=False,
                query_text=query_text
            )
            next_token_logits = outputs['logits'][:, -1, :]
            
            # åº”ç”¨é‡å¤æƒ©ç½šï¼šå¯¹å·²ç”Ÿæˆçš„tokené™ä½æ¦‚ç‡
            if repetition_penalty > 1.0 and len(generated_token_ids) > 0:
                # è·å–æœ€è¿‘ç”Ÿæˆçš„tokenï¼ˆç”¨äºæƒ©ç½šï¼‰
                recent_tokens = set(generated_token_ids[-10:])  # åªæƒ©ç½šæœ€è¿‘10ä¸ªtoken
                for token_id in recent_tokens:
                    if token_id < next_token_logits.size(-1):
                        # å¦‚æœlogitæ˜¯æ­£æ•°ï¼Œé™¤ä»¥penaltyï¼›å¦‚æœæ˜¯è´Ÿæ•°ï¼Œä¹˜ä»¥penalty
                        if next_token_logits[0, token_id] > 0:
                            next_token_logits[0, token_id] /= repetition_penalty
                        else:
                            next_token_logits[0, token_id] *= repetition_penalty
            
            # ç‰¹åˆ«å¤„ç†æ¢è¡Œç¬¦ï¼šå¤§å¹…é™ä½æ¢è¡Œç¬¦çš„æ¦‚ç‡ï¼ˆæ— è®ºæ˜¯å¦å·²ç»ç”Ÿæˆè¿‡ï¼‰
            if newline_token_ids:
                for nl_id in newline_token_ids:
                    if nl_id < next_token_logits.size(-1):
                        # å¯¹æ¢è¡Œç¬¦åº”ç”¨æ›´å¼ºçš„æƒ©ç½šï¼ˆåŸºç¡€æƒ©ç½š + è¿ç»­æƒ©ç½šï¼‰
                        penalty = newline_penalty * (1 + consecutive_newlines * 2)
                        if next_token_logits[0, nl_id] > 0:
                            next_token_logits[0, nl_id] /= penalty
                        else:
                            next_token_logits[0, nl_id] *= penalty
            
            # åº”ç”¨æ¸©åº¦ç¼©æ”¾
            scaled_logits = next_token_logits / temperature
            
            # æ£€æŸ¥2-gramé‡å¤ï¼ˆå¦‚æœå½“å‰åºåˆ—çš„æœ€å1ä¸ªtokenå’Œå€™é€‰tokenå½¢æˆé‡å¤ï¼‰
            if no_repeat_ngram_size > 0 and len(generated_token_ids) >= no_repeat_ngram_size - 1:
                # è·å–æœ€è¿‘çš„n-1ä¸ªtoken
                recent_ngram = generated_token_ids[-(no_repeat_ngram_size-1):]
                # å¦‚æœä¸‹ä¸€ä¸ªtokenå’Œæœ€è¿‘çš„tokenå½¢æˆé‡å¤ï¼Œé™ä½å…¶æ¦‚ç‡
                if len(recent_ngram) > 0 and recent_ngram[-1] < scaled_logits.size(-1):
                    # å¯¹é‡å¤çš„tokenåº”ç”¨æ›´å¼ºçš„æƒ©ç½š
                    scaled_logits[0, recent_ngram[-1]] /= (repetition_penalty * 2)
            
            next_token = torch.argmax(scaled_logits, dim=-1, keepdim=True)
            generated.append(next_token)
            token_id = next_token.item()
            generated_token_ids.append(token_id)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¢è¡Œç¬¦
            is_newline = token_id in newline_token_ids
            if is_newline:
                consecutive_newlines += 1
                # å¦‚æœè¿ç»­ç”Ÿæˆå¤ªå¤šæ¢è¡Œç¬¦ï¼ˆè¶…è¿‡2ä¸ªï¼‰ï¼Œå¼ºåˆ¶åœæ­¢
                if consecutive_newlines > 2:
                    break
            else:
                consecutive_newlines = 0
            
            current_input = torch.cat([current_input, next_token], dim=1)
            
            # åªæ£€æŸ¥EOSåœæ­¢æ¡ä»¶
            if token_id == eos_token_id:
                break
        
        generated_ids = torch.cat(generated, dim=1) if generated else input_ids
        # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        new_tokens = generated_ids[0][len(input_ids[0]):]
        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
    else:
        # åŸºç¡€æ¨¡å‹ä½¿ç”¨æ ‡å‡†generate
        generated_ids = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        if generated_text.startswith(prompt):
            generated_text = generated_text[len(prompt):].strip()
    
    return generated_text

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot_product / (norm1 * norm2)

def compare_sentences(draft_model, target_model, tokenizer, question: str, 
                     max_new_tokens: int = 50, knowledge_cache_manager=None,
                     embedding_model=None, temperature: float = 0.7):
    """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ç”Ÿæˆçš„å¥å­"""
    print(f"\n{'='*70}")
    print(f"é—®é¢˜: {question}")
    print(f"{'='*70}")
    
    # è‰ç¨¿æ¨¡å‹ç”Ÿæˆ
    print("\nğŸ“ è‰ç¨¿æ¨¡å‹ç”Ÿæˆä¸­...")
    draft_text = generate_full_sentence(
        draft_model, tokenizer, question, 
        max_new_tokens=max_new_tokens,
        knowledge_cache_manager=knowledge_cache_manager,
        query_text=question,
        temperature=temperature
    )
    print(f"è‰ç¨¿æ¨¡å‹è¾“å‡º: {draft_text}")
    
    # åŸºç¡€æ¨¡å‹ç”Ÿæˆ
    print("\nğŸ“ åŸºç¡€æ¨¡å‹ç”Ÿæˆä¸­...")
    target_text = generate_full_sentence(
        target_model, tokenizer, question,
        max_new_tokens=max_new_tokens
    )
    print(f"åŸºç¡€æ¨¡å‹è¾“å‡º: {target_text}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    if embedding_model is not None:
        try:
            emb1 = embedding_model.encode(draft_text, convert_to_numpy=True)
            emb2 = embedding_model.encode(target_text, convert_to_numpy=True)
            similarity = cosine_similarity(emb1, emb2)
            similarity_method = "å‘é‡ç›¸ä¼¼åº¦ï¼ˆembeddingï¼‰"
        except Exception as e:
            print(f"âš  å‘é‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            similarity = 0.0
            similarity_method = "è®¡ç®—å¤±è´¥"
    else:
        similarity = 0.0
        similarity_method = "éœ€è¦embeddingæ¨¡å‹"
    
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦åˆ†æ:")
    print(f"  æ–¹æ³•: {similarity_method}")
    print(f"  ç›¸ä¼¼åº¦: {similarity:.4f} ({similarity*100:.2f}%)")
    
    return {
        'question': question,
        'draft_text': draft_text,
        'target_text': target_text,
        'similarity': similarity,
        'similarity_method': similarity_method
    }

def main():
    """ä¸»å‡½æ•°"""
    config_path = "configs/qwen3_0.6b_config.yaml"
    
    # åŠ è½½æ¨¡å‹
    draft_model, target_model, tokenizer, knowledge_cache_manager = load_models(config_path)
    
    # åŠ è½½embeddingæ¨¡å‹ç”¨äºç›¸ä¼¼åº¦è®¡ç®—
    embedding_model = None
    if SENTENCE_TRANSFORMERS_AVAILABLE:
        print("\n5. åŠ è½½embeddingæ¨¡å‹ç”¨äºç›¸ä¼¼åº¦è®¡ç®—...")
        try:
            embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ“ Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"âš  æ— æ³•åŠ è½½embeddingæ¨¡å‹: {e}")
    
    # æµ‹è¯•é—®é¢˜ï¼ˆè®­ç»ƒé›†:éè®­ç»ƒé›† = 4:1ï¼‰
    # è®­ç»ƒé›†é—®é¢˜ï¼ˆæ¥è‡ªçŸ¥è¯†åº“ï¼Œ8ä¸ªï¼Œ80%ï¼‰
    training_questions = [
        "æ·±åº¦å­¦ä¹ æ˜¯",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯",
        "è®¡ç®—æœºè§†è§‰æ˜¯",
        "å¼ºåŒ–å­¦ä¹ æ˜¯",
        "Transformeræ¶æ„æ˜¯",
        "æœºå™¨å­¦ä¹ æ˜¯",
        "æ³¨æ„åŠ›æœºåˆ¶æ˜¯",
        "ç¥ç»ç½‘ç»œæ˜¯",
    ]
    
    # éè®­ç»ƒé›†é—®é¢˜ï¼ˆä¸åœ¨çŸ¥è¯†åº“ä¸­ï¼Œ2ä¸ªï¼Œ20%ï¼‰
    non_training_questions = [
        "é‡å­è®¡ç®—æ˜¯",
        "è¾¹ç¼˜è®¡ç®—æ˜¯",
    ]
    
    questions = training_questions + non_training_questions
    
    print("\n" + "="*70)
    print("å®Œæ•´å¥å­ç”Ÿæˆä¸ç›¸ä¼¼åº¦æ¯”è¾ƒ")
    print("="*70)
    print(f"\næµ‹è¯•é—®é¢˜æ•°é‡: {len(questions)}")
    print(f"  - è®­ç»ƒé›†é—®é¢˜: {len(training_questions)} ä¸ª (80%)")
    print(f"  - éè®­ç»ƒé›†é—®é¢˜: {len(non_training_questions)} ä¸ª (20%)")
    print(f"æ¯ä¸ªæ¨¡å‹æœ€å¤šç”Ÿæˆ: 50 tokens")
    
    # ä½¿ç”¨æ¸©åº¦ç¼©æ”¾ï¼ˆ0.7ä½¿åˆ†å¸ƒæ›´å°–é”ï¼Œæé«˜ç¡®å®šæ€§ï¼‰
    temperature = 0.7
    print(f"\nä½¿ç”¨æ¸©åº¦ç¼©æ”¾: {temperature} (é™ä½æ¸©åº¦ä½¿åˆ†å¸ƒæ›´å°–é”ï¼Œæé«˜æ¥å—ç‡)")
    
    all_results = []
    
    for question in questions:
        result = compare_sentences(
            draft_model, target_model, tokenizer, question,
            max_new_tokens=50,
            knowledge_cache_manager=knowledge_cache_manager,
            embedding_model=embedding_model,
            temperature=temperature
        )
        all_results.append(result)
    
    # æ€»ä½“æ±‡æ€»
    print("\n\n" + "="*70)
    print("æ€»ä½“æ±‡æ€»")
    print("="*70)
    
    similarities = [r['similarity'] for r in all_results]
    avg_similarity = np.mean(similarities)
    median_similarity = np.median(similarities)
    
    print(f"\nå¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f} ({avg_similarity*100:.2f}%)")
    print(f"ç›¸ä¼¼åº¦ä¸­ä½æ•°: {median_similarity:.4f} ({median_similarity*100:.2f}%)")
    print(f"æœ€é«˜ç›¸ä¼¼åº¦: {max(similarities):.4f} ({max(similarities)*100:.2f}%)")
    print(f"æœ€ä½ç›¸ä¼¼åº¦: {min(similarities):.4f} ({min(similarities)*100:.2f}%)")
    
    print(f"\nå„é—®é¢˜è¯¦ç»†ç»“æœ:")
    print("\nã€è®­ç»ƒé›†é—®é¢˜ã€‘:")
    for i, r in enumerate(all_results[:len(training_questions)], 1):
        print(f"\n  {i}. {r['question']}")
        print(f"     è‰ç¨¿æ¨¡å‹: {r['draft_text'][:80]}...")
        print(f"     åŸºç¡€æ¨¡å‹: {r['target_text'][:80]}...")
        print(f"     ç›¸ä¼¼åº¦: {r['similarity']:.4f} ({r['similarity']*100:.2f}%)")
    
    print("\nã€éè®­ç»ƒé›†é—®é¢˜ã€‘:")
    for i, r in enumerate(all_results[len(training_questions):], 1):
        print(f"\n  {i}. {r['question']}")
        print(f"     è‰ç¨¿æ¨¡å‹: {r['draft_text'][:80]}...")
        print(f"     åŸºç¡€æ¨¡å‹: {r['target_text'][:80]}...")
        print(f"     ç›¸ä¼¼åº¦: {r['similarity']:.4f} ({r['similarity']*100:.2f}%)")
    
    # åˆ†åˆ«ç»Ÿè®¡è®­ç»ƒé›†å’Œéè®­ç»ƒé›†çš„è¡¨ç°
    training_results = all_results[:len(training_questions)]
    non_training_results = all_results[len(training_questions):]
    
    training_similarities = [r['similarity'] for r in training_results]
    non_training_similarities = [r['similarity'] for r in non_training_results]
    
    print(f"\nã€åˆ†ç±»ç»Ÿè®¡ã€‘:")
    print(f"  è®­ç»ƒé›†é—®é¢˜å¹³å‡ç›¸ä¼¼åº¦: {np.mean(training_similarities):.4f} ({np.mean(training_similarities)*100:.2f}%)")
    print(f"  éè®­ç»ƒé›†é—®é¢˜å¹³å‡ç›¸ä¼¼åº¦: {np.mean(non_training_similarities):.4f} ({np.mean(non_training_similarities)*100:.2f}%)")
    
    print("\n" + "="*70)
    print("åˆ†æå®Œæˆï¼")
    print("="*70)

if __name__ == "__main__":
    main()

