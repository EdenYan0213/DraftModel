#!/usr/bin/env python3
"""
æ¯”è¾ƒè‰ç¨¿æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´å¥å­ç›¸ä¼¼åº¦
"""

import os
import sys
import torch
import yaml
from pathlib import Path
from typing import List, Dict
import numpy as np

sys.path.insert(0, str(Path(__file__).parent.parent))

from models.base_loader import Qwen3Loader
from models.knowledge_enhanced_draft import Qwen3DraftModel
from models.knowledge_cache import KnowledgeCacheManager

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("è­¦å‘Š: sentence-transformers æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦")

def load_models(config_path: str, checkpoint_path: str = None):
    """åŠ è½½æ¨¡å‹"""
    print("="*70)
    print("åŠ è½½æ¨¡å‹")
    print("="*70)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("\n1. åŠ è½½åŸºç¡€æ¨¡å‹...")
    loader = Qwen3Loader(config_path)
    target_model = loader.load_target_model(device='cpu')
    tokenizer = loader.load_tokenizer()
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½çŸ¥è¯†ç¼“å­˜
    print("\n2. åŠ è½½çŸ¥è¯†ç¼“å­˜...")
    cache_path = "output/knowledge_cache/knowledge_cache.pth"
    knowledge_cache_manager = None
    
    if os.path.exists(cache_path):
        cache_data = torch.load(cache_path, map_location='cpu', weights_only=False)
        
        knowledge_config = config.get('knowledge_enhancement', {})
        use_vector_retrieval = knowledge_config.get('use_vector_retrieval', True)
        embedding_model_name = knowledge_config.get('embedding_model_name', None)
        
        knowledge_cache_manager = KnowledgeCacheManager(
            hidden_size=config['base_model']['hidden_size'],
            num_heads=config['base_model']['num_attention_heads'],
            cache_dim=config['knowledge_enhancement']['cache_dim'],
            use_vector_retrieval=use_vector_retrieval,
            embedding_model_name=embedding_model_name,
            target_model=target_model,  # ä½¿ç”¨ç›®æ ‡æ¨¡å‹çš„åµŒå…¥å±‚
            tokenizer=tokenizer  # ä¼ å…¥tokenizer
        )
        knowledge_cache_manager.kv_cache = cache_data.get('kv_cache', {})
        
        if 'knowledge_embeddings' in cache_data:
            knowledge_cache_manager.knowledge_embeddings = cache_data['knowledge_embeddings']
        
        print(f"âœ“ çŸ¥è¯†ç¼“å­˜åŠ è½½å®Œæˆï¼Œå…± {len(knowledge_cache_manager.kv_cache)} ä¸ªçŸ¥è¯†é¡¹")
    
    # åˆ›å»ºè‰ç¨¿æ¨¡å‹
    print("\n3. åˆ›å»ºè‰ç¨¿æ¨¡å‹...")
    draft_model = Qwen3DraftModel(config, target_model, knowledge_cache_manager=knowledge_cache_manager)
    draft_model = draft_model.cpu()
    draft_model.eval()
    target_model.eval()
    
    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡ï¼ˆä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„best checkpointï¼‰
    if checkpoint_path is None:
        checkpoint_dir = "output/checkpoints"
        # ä¼˜å…ˆæŸ¥æ‰¾best checkpointï¼ŒæŒ‰epochæ’åº
        knowledge_checkpoints = [f for f in os.listdir(checkpoint_dir) 
                                if 'knowledge' in f and 'best' in f and f.endswith('.pth')]
        if knowledge_checkpoints:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
            knowledge_checkpoints.sort(key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)), reverse=True)
            checkpoint_path = os.path.join(checkpoint_dir, knowledge_checkpoints[0])
            print(f"  è‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„checkpoint: {knowledge_checkpoints[0]}")
    
    if checkpoint_path and os.path.exists(checkpoint_path):
        print(f"\n4. åŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        # ä½¿ç”¨strict=Falseä»¥å…¼å®¹æ¨¡å‹ç»“æ„å˜åŒ–ï¼ˆå¦‚æ–°å¢çš„å½’ä¸€åŒ–å±‚ï¼‰
        missing_keys, unexpected_keys = draft_model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        print(f"âœ“ æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
        print(f"  - Epoch: {checkpoint.get('epoch', 'N/A')}")
        print(f"  - éªŒè¯æŸå¤±: {checkpoint.get('val_loss', 0):.4f}")
        if missing_keys:
            print(f"  âš  ç¼ºå¤±çš„é”®: {len(missing_keys)}ä¸ªï¼ˆå¯èƒ½æ˜¯æ–°å¢çš„å±‚ï¼‰")
        if unexpected_keys:
            print(f"  âš  å¤šä½™çš„é”®: {len(unexpected_keys)}ä¸ª")
    else:
        print(f"\nâš  æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨æœªè®­ç»ƒçš„æ¨¡å‹")
    
    return draft_model, target_model, tokenizer, knowledge_cache_manager

def generate_full_sentence(model, tokenizer, prompt: str, max_new_tokens: int = 50, 
                          knowledge_cache_manager=None, query_text=None, temperature: float = 0.7,
                          top_p: float = 0.9, use_top_p: bool = True, 
                          dynamic_repetition_penalty: bool = True):
    """ç”Ÿæˆå®Œæ•´å¥å­ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    
    Args:
        temperature: æ¸©åº¦ç¼©æ”¾å‚æ•°ï¼Œ<1ä½¿åˆ†å¸ƒæ›´å°–é”ï¼ˆæé«˜ç¡®å®šæ€§ï¼‰ï¼Œ>1ä½¿åˆ†å¸ƒæ›´å¹³æ»‘ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
        top_p: nucleusé‡‡æ ·å‚æ•°ï¼Œç´¯ç§¯æ¦‚ç‡é˜ˆå€¼ï¼ˆ0.9è¡¨ç¤ºä¿ç•™ç´¯ç§¯æ¦‚ç‡90%çš„tokenï¼‰
        use_top_p: æ˜¯å¦ä½¿ç”¨top-pé‡‡æ ·ï¼ˆTrueï¼‰æˆ–è´ªå¿ƒè§£ç ï¼ˆFalseï¼‰
        dynamic_repetition_penalty: æ˜¯å¦ä½¿ç”¨åŠ¨æ€é‡å¤æƒ©ç½šï¼ˆæ ¹æ®å·²ç”Ÿæˆé•¿åº¦è°ƒæ•´ï¼‰
    """
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs['input_ids']
    
    # å¦‚æœæ˜¯è‰ç¨¿æ¨¡å‹ï¼Œéœ€è¦ä¼ é€’çŸ¥è¯†ç¼“å­˜
    if isinstance(model, Qwen3DraftModel):
        # æ£€ç´¢çŸ¥è¯†ï¼ˆé™ä½é˜ˆå€¼ä»¥æé«˜æ£€ç´¢ç‡ï¼Œå¹¶è·å–ç›¸ä¼¼åº¦ï¼‰
        knowledge_cache = None
        knowledge_similarity = None
        if knowledge_cache_manager is not None and query_text is not None:
            knowledge_config = model.config.get('knowledge_enhancement', {})
            threshold = knowledge_config.get('retrieval_threshold', 0.5)  # ä»0.7é™ä½åˆ°0.5
            retrieved = knowledge_cache_manager.retrieve(query_text, threshold=threshold, return_similarity=True)
            if retrieved is not None:
                if len(retrieved) == 4:
                    # è¿”å› (keys, values, similarity, answer_start_idx)
                    knowledge_cache = (retrieved[0], retrieved[1])
                    knowledge_similarity = retrieved[2]
                    answer_start_idx = retrieved[3]
                    # è®¾ç½®æ¨¡å‹çš„ç›¸ä¼¼åº¦ã€å¼ºåˆ¶æƒé‡å’Œç­”æ¡ˆèµ·å§‹ä½ç½®
                    if hasattr(model, '_current_knowledge_similarity'):
                        model._current_knowledge_similarity = knowledge_similarity
                    if hasattr(model, '_current_answer_start_idx'):
                        model._current_answer_start_idx = answer_start_idx
                    if hasattr(model, '_force_knowledge_weight'):
                        if knowledge_similarity > 0.8:
                            model._force_knowledge_weight = 0.6  # è‡³å°‘60%ä½¿ç”¨çŸ¥è¯†ï¼ˆæ›´æ¸©å’Œï¼‰
                        elif knowledge_similarity > 0.6:
                            model._force_knowledge_weight = 0.4  # è‡³å°‘40%ä½¿ç”¨çŸ¥è¯†
                        else:
                            model._force_knowledge_weight = None  # ä¸å¼ºåˆ¶ï¼Œä½¿ç”¨ç›¸ä¼¼åº¦è°ƒæ•´
                elif len(retrieved) == 3:
                    # å¯èƒ½æ˜¯ (keys, values, similarity) æˆ– (keys, values, answer_start_idx)
                    if isinstance(retrieved[2], float):
                        knowledge_cache = (retrieved[0], retrieved[1])
                        knowledge_similarity = retrieved[2]
                        if hasattr(model, '_current_knowledge_similarity'):
                            model._current_knowledge_similarity = knowledge_similarity
                        if hasattr(model, '_current_answer_start_idx'):
                            model._current_answer_start_idx = None
                    else:
                        knowledge_cache = (retrieved[0], retrieved[1])
                        if hasattr(model, '_current_answer_start_idx'):
                            model._current_answer_start_idx = retrieved[2]
                else:
                    knowledge_cache = retrieved
        
        # ç”Ÿæˆtokenï¼ˆæ·»åŠ é‡å¤æƒ©ç½šï¼Œç‰¹åˆ«å¤„ç†æ¢è¡Œç¬¦ï¼‰
        current_input = input_ids
        generated = []
        eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id
        
        # é‡å¤æƒ©ç½šå‚æ•°ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
        base_repetition_penalty = 1.2  # åŸºç¡€é‡å¤æƒ©ç½š
        no_repeat_ngram_size = 3  # ç¦æ­¢3-gramé‡å¤ï¼ˆä»2å¢åŠ åˆ°3ï¼‰
        newline_penalty = 3.0  # å¯¹æ¢è¡Œç¬¦çš„é¢å¤–æƒ©ç½šï¼ˆå¢å¼ºï¼‰
        
        # åŠ¨æ€é‡å¤æƒ©ç½šï¼šæ ¹æ®å·²ç”Ÿæˆé•¿åº¦è°ƒæ•´æƒ©ç½šå¼ºåº¦
        def get_repetition_penalty(generated_length: int) -> float:
            """åŠ¨æ€è°ƒæ•´é‡å¤æƒ©ç½šï¼šç”Ÿæˆé•¿åº¦è¶Šé•¿ï¼Œæƒ©ç½šè¶Šå¼º"""
            if not dynamic_repetition_penalty:
                return base_repetition_penalty
            # çº¿æ€§å¢åŠ ï¼šæ¯10ä¸ªtokenå¢åŠ 0.05çš„æƒ©ç½š
            penalty_increase = (generated_length // 10) * 0.05
            return min(base_repetition_penalty + penalty_increase, 1.5)  # æœ€å¤§1.5
        
        # è¯†åˆ«æ¢è¡Œç¬¦token ID
        newline_token_ids = set()
        for test_str in ['\n', '\r\n', '\r']:
            try:
                token_ids = tokenizer.encode(test_str, add_special_tokens=False)
                newline_token_ids.update(token_ids)
            except:
                pass
        
        # ç”¨äºè·Ÿè¸ªå·²ç”Ÿæˆçš„tokenï¼ˆç”¨äºé‡å¤æƒ©ç½šï¼‰
        generated_token_ids = []
        consecutive_newlines = 0  # è¿ç»­æ¢è¡Œç¬¦è®¡æ•°
        
        # è®¡ç®—é—®é¢˜çš„é•¿åº¦ï¼ˆç”¨äºè·Ÿè¸ªç­”æ¡ˆç”Ÿæˆä½ç½®ï¼‰
        question_length = input_ids.shape[1]
        
        # è®¾ç½®æ¨¡å‹çš„æŸ¥è¯¢é•¿åº¦ï¼ˆç”¨äºåŠ¨æ€maskï¼‰
        if hasattr(model, '_current_query_length'):
            model._current_query_length = question_length
        
        for i in range(max_new_tokens):
            # è®¡ç®—å½“å‰å·²ç”Ÿæˆçš„ç­”æ¡ˆé•¿åº¦ï¼ˆä¸åŒ…æ‹¬é—®é¢˜éƒ¨åˆ†ï¼‰
            current_answer_length = current_input.shape[1] - question_length
            # è®¾ç½®æ¨¡å‹çš„å½“å‰ç­”æ¡ˆé•¿åº¦ï¼ˆç”¨äºKVå¯¹é½ï¼‰
            if hasattr(model, '_current_answer_length'):
                model._current_answer_length = current_answer_length
            
            outputs = model.forward(
                current_input, 
                knowledge_cache=knowledge_cache,
                retrieve_knowledge=False,
                query_text=query_text
            )
            next_token_logits = outputs['logits'][:, -1, :]
            
            # åº”ç”¨åŠ¨æ€é‡å¤æƒ©ç½šï¼šå¯¹å·²ç”Ÿæˆçš„tokené™ä½æ¦‚ç‡
            current_repetition_penalty = get_repetition_penalty(len(generated_token_ids))
            if current_repetition_penalty > 1.0 and len(generated_token_ids) > 0:
                # è·å–æœ€è¿‘ç”Ÿæˆçš„tokenï¼ˆç”¨äºæƒ©ç½šï¼Œçª—å£å¤§å°ä¹ŸåŠ¨æ€è°ƒæ•´ï¼‰
                window_size = min(15, len(generated_token_ids))  # æœ€å¤šæƒ©ç½šæœ€è¿‘15ä¸ªtoken
                recent_tokens = set(generated_token_ids[-window_size:])
                for token_id in recent_tokens:
                    if token_id < next_token_logits.size(-1):
                        # å¦‚æœlogitæ˜¯æ­£æ•°ï¼Œé™¤ä»¥penaltyï¼›å¦‚æœæ˜¯è´Ÿæ•°ï¼Œä¹˜ä»¥penalty
                        if next_token_logits[0, token_id] > 0:
                            next_token_logits[0, token_id] /= current_repetition_penalty
                        else:
                            next_token_logits[0, token_id] *= current_repetition_penalty
            
            # ç‰¹åˆ«å¤„ç†æ¢è¡Œç¬¦ï¼šå¤§å¹…é™ä½æ¢è¡Œç¬¦çš„æ¦‚ç‡ï¼ˆæ— è®ºæ˜¯å¦å·²ç»ç”Ÿæˆè¿‡ï¼‰
            if newline_token_ids:
                for nl_id in newline_token_ids:
                    if nl_id < next_token_logits.size(-1):
                        # å¯¹æ¢è¡Œç¬¦åº”ç”¨æ›´å¼ºçš„æƒ©ç½šï¼ˆåŸºç¡€æƒ©ç½š + è¿ç»­æƒ©ç½šï¼‰
                        penalty = newline_penalty * (1 + consecutive_newlines * 2)
                        if next_token_logits[0, nl_id] > 0:
                            next_token_logits[0, nl_id] /= penalty
                        else:
                            next_token_logits[0, nl_id] *= penalty
            
            # åº”ç”¨æ¸©åº¦ç¼©æ”¾
            scaled_logits = next_token_logits / temperature
            
            # æ£€æŸ¥n-gramé‡å¤ï¼ˆé˜²æ­¢çŸ­è¯­é‡å¤ï¼‰
            if no_repeat_ngram_size > 0 and len(generated_token_ids) >= no_repeat_ngram_size - 1:
                # è·å–æœ€è¿‘çš„n-1ä¸ªtoken
                recent_ngram = generated_token_ids[-(no_repeat_ngram_size-1):]
                # æ£€æŸ¥æ˜¯å¦å½¢æˆé‡å¤çš„n-gram
                for candidate_id in range(scaled_logits.size(-1)):
                    test_ngram = recent_ngram + [candidate_id]
                    # æ£€æŸ¥è¿™ä¸ªn-gramæ˜¯å¦åœ¨å†å²ä¸­å‡ºç°è¿‡
                    if len(generated_token_ids) >= no_repeat_ngram_size:
                        for i in range(len(generated_token_ids) - no_repeat_ngram_size + 1):
                            if generated_token_ids[i:i+no_repeat_ngram_size] == test_ngram:
                                # å¦‚æœå½¢æˆé‡å¤ï¼Œå¤§å¹…é™ä½æ¦‚ç‡
                                scaled_logits[0, candidate_id] /= (current_repetition_penalty * 3)
                                break
            
            # Top-p (nucleus) é‡‡æ ·æˆ–è´ªå¿ƒè§£ç 
            if use_top_p and top_p < 1.0:
                # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
                probs = torch.softmax(scaled_logits, dim=-1)
                
                # æŒ‰æ¦‚ç‡æ’åº
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                
                # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                
                # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
                sorted_indices_to_remove = cumulative_probs > top_p
                
                # ä¿ç•™ç¬¬ä¸€ä¸ªè¶…è¿‡é˜ˆå€¼çš„tokenï¼ˆç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªtokenå¯é€‰ï¼‰
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = False
                
                # å°†è¶…å‡ºèŒƒå›´çš„tokenæ¦‚ç‡è®¾ä¸º0
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                probs[indices_to_remove] = 0
                
                # é‡æ–°å½’ä¸€åŒ–
                probs = probs / probs.sum(dim=-1, keepdim=True)
                
                # ä»è¿‡æ»¤åçš„åˆ†å¸ƒä¸­é‡‡æ ·
                next_token = torch.multinomial(probs, num_samples=1)
            else:
                # è´ªå¿ƒè§£ç 
                next_token = torch.argmax(scaled_logits, dim=-1, keepdim=True)
            generated.append(next_token)
            token_id = next_token.item()
            generated_token_ids.append(token_id)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¢è¡Œç¬¦
            is_newline = token_id in newline_token_ids
            if is_newline:
                consecutive_newlines += 1
                # å¦‚æœè¿ç»­ç”Ÿæˆå¤ªå¤šæ¢è¡Œç¬¦ï¼ˆè¶…è¿‡2ä¸ªï¼‰ï¼Œå¼ºåˆ¶åœæ­¢
                if consecutive_newlines > 2:
                    break
            else:
                consecutive_newlines = 0
            
            current_input = torch.cat([current_input, next_token], dim=1)
            
            # æ”¹è¿›çš„åœæ­¢æ¡ä»¶
            if token_id == eos_token_id:
                break
        
        generated_ids = torch.cat(generated, dim=1) if generated else input_ids
        # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        new_tokens = generated_ids[0][len(input_ids[0]):]
        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
    else:
        # åŸºç¡€æ¨¡å‹ä½¿ç”¨æ ‡å‡†generate
        generated_ids = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        if generated_text.startswith(prompt):
            generated_text = generated_text[len(prompt):].strip()
    
    return generated_text

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot_product / (norm1 * norm2)

def compare_sentences(draft_model, target_model, tokenizer, question: str, 
                     max_new_tokens: int = 50, knowledge_cache_manager=None,
                     embedding_model=None, temperature: float = 0.7):
    """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ç”Ÿæˆçš„å¥å­"""
    print(f"\n{'='*70}")
    print(f"é—®é¢˜: {question}")
    print(f"{'='*70}")
    
    # è‰ç¨¿æ¨¡å‹ç”Ÿæˆï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„ç”Ÿæˆç­–ç•¥ï¼‰
    print("\nğŸ“ è‰ç¨¿æ¨¡å‹ç”Ÿæˆä¸­...")
    draft_text = generate_full_sentence(
        draft_model, tokenizer, question, 
        max_new_tokens=max_new_tokens,
        knowledge_cache_manager=knowledge_cache_manager,
        query_text=question,
        temperature=temperature,
        top_p=0.9,  # ä½¿ç”¨top-pé‡‡æ ·
        use_top_p=True,  # å¯ç”¨top-pé‡‡æ ·
        dynamic_repetition_penalty=True  # å¯ç”¨åŠ¨æ€é‡å¤æƒ©ç½š
    )
    print(f"è‰ç¨¿æ¨¡å‹è¾“å‡º: {draft_text}")
    
    # åŸºç¡€æ¨¡å‹ç”Ÿæˆ
    print("\nğŸ“ åŸºç¡€æ¨¡å‹ç”Ÿæˆä¸­...")
    target_text = generate_full_sentence(
        target_model, tokenizer, question,
        max_new_tokens=max_new_tokens
    )
    print(f"åŸºç¡€æ¨¡å‹è¾“å‡º: {target_text}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦ - ä¼˜å…ˆä½¿ç”¨embeddingæ¨¡å‹è·å–è¯­ä¹‰å‘é‡
    similarity = 0.0
    similarity_method = "è®¡ç®—å¤±è´¥"
    
    # ä¼˜å…ˆä½¿ç”¨embeddingæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if embedding_model is not None:
        try:
            # ä½¿ç”¨embeddingæ¨¡å‹è·å–ä¸¤å¥è¯çš„è¯­ä¹‰å‘é‡
            draft_embedding = embedding_model.encode(draft_text, convert_to_numpy=True)
            target_embedding = embedding_model.encode(target_text, convert_to_numpy=True)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity(draft_embedding, target_embedding)
            similarity_method = "è¯­ä¹‰å‘é‡ç›¸ä¼¼åº¦ï¼ˆembeddingæ¨¡å‹ï¼‰"
        except Exception as e:
            print(f"âš  Embeddingæ¨¡å‹è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å›é€€åˆ°ç›®æ ‡æ¨¡å‹çš„hidden states
            try:
                device = next(target_model.parameters()).device
                
                # å¯¹è‰ç¨¿æ¨¡å‹è¾“å‡ºè¿›è¡Œç¼–ç 
                draft_inputs = tokenizer(draft_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                draft_input_ids = draft_inputs['input_ids'].to(device)
                
                # å¯¹åŸºç¡€æ¨¡å‹è¾“å‡ºè¿›è¡Œç¼–ç 
                target_inputs = tokenizer(target_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                target_input_ids = target_inputs['input_ids'].to(device)
                
                with torch.no_grad():
                    # è·å–è‰ç¨¿æ¨¡å‹è¾“å‡ºçš„hidden statesï¼ˆæœ€åä¸€å±‚çš„è¾“å‡ºï¼‰
                    draft_outputs = target_model(input_ids=draft_input_ids, output_hidden_states=True)
                    draft_hidden_states = draft_outputs.hidden_states[-1]  # æœ€åä¸€å±‚çš„hidden states
                    
                    # è·å–åŸºç¡€æ¨¡å‹è¾“å‡ºçš„hidden statesï¼ˆæœ€åä¸€å±‚çš„è¾“å‡ºï¼‰
                    target_outputs = target_model(input_ids=target_input_ids, output_hidden_states=True)
                    target_hidden_states = target_outputs.hidden_states[-1]  # æœ€åä¸€å±‚çš„hidden states
                    
                    # ä½¿ç”¨å¹³å‡æ± åŒ–å¾—åˆ°å¥å­çº§è¡¨ç¤ºï¼ˆå›é€€æ–¹æ¡ˆï¼‰
                    draft_embedding = draft_hidden_states.mean(dim=1).squeeze(0).cpu().numpy()
                    target_embedding = target_hidden_states.mean(dim=1).squeeze(0).cpu().numpy()
                    
                    similarity = cosine_similarity(draft_embedding, target_embedding)
                    similarity_method = "è¯­ä¹‰å‘é‡ç›¸ä¼¼åº¦ï¼ˆç›®æ ‡æ¨¡å‹hidden statesï¼Œå›é€€ï¼‰"
            except Exception as e2:
                print(f"âš  ç›®æ ‡æ¨¡å‹hidden statesè®¡ç®—ä¹Ÿå¤±è´¥: {e2}")
                similarity = 0.0
                similarity_method = "è®¡ç®—å¤±è´¥"
    else:
        # å¦‚æœæ²¡æœ‰embeddingæ¨¡å‹ï¼Œå›é€€åˆ°ç›®æ ‡æ¨¡å‹çš„hidden states
        try:
            device = next(target_model.parameters()).device
            
            # å¯¹è‰ç¨¿æ¨¡å‹è¾“å‡ºè¿›è¡Œç¼–ç 
            draft_inputs = tokenizer(draft_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            draft_input_ids = draft_inputs['input_ids'].to(device)
            
            # å¯¹åŸºç¡€æ¨¡å‹è¾“å‡ºè¿›è¡Œç¼–ç 
            target_inputs = tokenizer(target_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            target_input_ids = target_inputs['input_ids'].to(device)
            
            with torch.no_grad():
                # è·å–è‰ç¨¿æ¨¡å‹è¾“å‡ºçš„hidden statesï¼ˆæœ€åä¸€å±‚çš„è¾“å‡ºï¼‰
                draft_outputs = target_model(input_ids=draft_input_ids, output_hidden_states=True)
                draft_hidden_states = draft_outputs.hidden_states[-1]  # æœ€åä¸€å±‚çš„hidden states
                
                # è·å–åŸºç¡€æ¨¡å‹è¾“å‡ºçš„hidden statesï¼ˆæœ€åä¸€å±‚çš„è¾“å‡ºï¼‰
                target_outputs = target_model(input_ids=target_input_ids, output_hidden_states=True)
                target_hidden_states = target_outputs.hidden_states[-1]  # æœ€åä¸€å±‚çš„hidden states
                
                # ä½¿ç”¨å¹³å‡æ± åŒ–å¾—åˆ°å¥å­çº§è¡¨ç¤º
                draft_embedding = draft_hidden_states.mean(dim=1).squeeze(0).cpu().numpy()
                target_embedding = target_hidden_states.mean(dim=1).squeeze(0).cpu().numpy()
                
                similarity = cosine_similarity(draft_embedding, target_embedding)
                similarity_method = "è¯­ä¹‰å‘é‡ç›¸ä¼¼åº¦ï¼ˆç›®æ ‡æ¨¡å‹hidden statesï¼Œæ— embeddingæ¨¡å‹ï¼‰"
        except Exception as e:
            print(f"âš  è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            similarity = 0.0
            similarity_method = "è®¡ç®—å¤±è´¥"
    
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦åˆ†æ:")
    print(f"  æ–¹æ³•: {similarity_method}")
    print(f"  ç›¸ä¼¼åº¦: {similarity:.4f} ({similarity*100:.2f}%)")
    
    return {
        'question': question,
        'draft_text': draft_text,
        'target_text': target_text,
        'similarity': similarity,
        'similarity_method': similarity_method
    }

def main():
    """ä¸»å‡½æ•°"""
    config_path = "configs/qwen3_0.6b_config.yaml"
    
    # åŠ è½½æ¨¡å‹
    draft_model, target_model, tokenizer, knowledge_cache_manager = load_models(config_path)
    
    # åŠ è½½embeddingæ¨¡å‹ç”¨äºç›¸ä¼¼åº¦è®¡ç®—
    embedding_model = None
    if SENTENCE_TRANSFORMERS_AVAILABLE:
        print("\n5. åŠ è½½embeddingæ¨¡å‹ç”¨äºç›¸ä¼¼åº¦è®¡ç®—...")
        try:
            embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ“ Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"âš  æ— æ³•åŠ è½½embeddingæ¨¡å‹: {e}")
    
    # æµ‹è¯•é—®é¢˜ï¼ˆè®­ç»ƒé›†:éè®­ç»ƒé›† = 4:1ï¼‰
    # è®­ç»ƒé›†é—®é¢˜ï¼ˆæ¥è‡ªçŸ¥è¯†åº“ï¼Œ8ä¸ªï¼Œ80%ï¼‰
    training_questions = [
        "æ·±åº¦å­¦ä¹ æ˜¯",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯",
        "è®¡ç®—æœºè§†è§‰æ˜¯",
        "å¼ºåŒ–å­¦ä¹ æ˜¯",
        "Transformeræ¶æ„æ˜¯",
        "æœºå™¨å­¦ä¹ æ˜¯",
        "æ³¨æ„åŠ›æœºåˆ¶æ˜¯",
        "ç¥ç»ç½‘ç»œæ˜¯",
    ]
    
    # éè®­ç»ƒé›†é—®é¢˜ï¼ˆä¸åœ¨çŸ¥è¯†åº“ä¸­ï¼Œ2ä¸ªï¼Œ20%ï¼‰
    non_training_questions = [
        "é‡å­è®¡ç®—æ˜¯",
        "è¾¹ç¼˜è®¡ç®—æ˜¯",
    ]
    
    questions = training_questions + non_training_questions
    
    print("\n" + "="*70)
    print("å®Œæ•´å¥å­ç”Ÿæˆä¸ç›¸ä¼¼åº¦æ¯”è¾ƒ")
    print("="*70)
    print(f"\næµ‹è¯•é—®é¢˜æ•°é‡: {len(questions)}")
    print(f"  - è®­ç»ƒé›†é—®é¢˜: {len(training_questions)} ä¸ª (80%)")
    print(f"  - éè®­ç»ƒé›†é—®é¢˜: {len(non_training_questions)} ä¸ª (20%)")
    print(f"æ¯ä¸ªæ¨¡å‹æœ€å¤šç”Ÿæˆ: 50 tokens")
    
    # ä½¿ç”¨æ¸©åº¦ç¼©æ”¾ï¼ˆ0.7ä½¿åˆ†å¸ƒæ›´å°–é”ï¼Œæé«˜ç¡®å®šæ€§ï¼‰
    temperature = 0.7
    print(f"\nä½¿ç”¨æ¸©åº¦ç¼©æ”¾: {temperature} (é™ä½æ¸©åº¦ä½¿åˆ†å¸ƒæ›´å°–é”ï¼Œæé«˜æ¥å—ç‡)")
    
    all_results = []
    
    for question in questions:
        result = compare_sentences(
            draft_model, target_model, tokenizer, question,
            max_new_tokens=50,
            knowledge_cache_manager=knowledge_cache_manager,
            embedding_model=embedding_model,
            temperature=temperature
        )
        all_results.append(result)
    
    # æ€»ä½“æ±‡æ€»
    print("\n\n" + "="*70)
    print("æ€»ä½“æ±‡æ€»")
    print("="*70)
    
    similarities = [r['similarity'] for r in all_results]
    avg_similarity = np.mean(similarities)
    median_similarity = np.median(similarities)
    
    print(f"\nå¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f} ({avg_similarity*100:.2f}%)")
    print(f"ç›¸ä¼¼åº¦ä¸­ä½æ•°: {median_similarity:.4f} ({median_similarity*100:.2f}%)")
    print(f"æœ€é«˜ç›¸ä¼¼åº¦: {max(similarities):.4f} ({max(similarities)*100:.2f}%)")
    print(f"æœ€ä½ç›¸ä¼¼åº¦: {min(similarities):.4f} ({min(similarities)*100:.2f}%)")
    
    print(f"\nå„é—®é¢˜è¯¦ç»†ç»“æœ:")
    print("\nã€è®­ç»ƒé›†é—®é¢˜ã€‘:")
    for i, r in enumerate(all_results[:len(training_questions)], 1):
        print(f"\n  {i}. {r['question']}")
        print(f"     è‰ç¨¿æ¨¡å‹: {r['draft_text'][:80]}...")
        print(f"     åŸºç¡€æ¨¡å‹: {r['target_text'][:80]}...")
        print(f"     ç›¸ä¼¼åº¦: {r['similarity']:.4f} ({r['similarity']*100:.2f}%)")
    
    print("\nã€éè®­ç»ƒé›†é—®é¢˜ã€‘:")
    for i, r in enumerate(all_results[len(training_questions):], 1):
        print(f"\n  {i}. {r['question']}")
        print(f"     è‰ç¨¿æ¨¡å‹: {r['draft_text'][:80]}...")
        print(f"     åŸºç¡€æ¨¡å‹: {r['target_text'][:80]}...")
        print(f"     ç›¸ä¼¼åº¦: {r['similarity']:.4f} ({r['similarity']*100:.2f}%)")
    
    # åˆ†åˆ«ç»Ÿè®¡è®­ç»ƒé›†å’Œéè®­ç»ƒé›†çš„è¡¨ç°
    training_results = all_results[:len(training_questions)]
    non_training_results = all_results[len(training_questions):]
    
    training_similarities = [r['similarity'] for r in training_results]
    non_training_similarities = [r['similarity'] for r in non_training_results]
    
    print(f"\nã€åˆ†ç±»ç»Ÿè®¡ã€‘:")
    print(f"  è®­ç»ƒé›†é—®é¢˜å¹³å‡ç›¸ä¼¼åº¦: {np.mean(training_similarities):.4f} ({np.mean(training_similarities)*100:.2f}%)")
    print(f"  éè®­ç»ƒé›†é—®é¢˜å¹³å‡ç›¸ä¼¼åº¦: {np.mean(non_training_similarities):.4f} ({np.mean(non_training_similarities)*100:.2f}%)")
    
    print("\n" + "="*70)
    print("åˆ†æå®Œæˆï¼")
    print("="*70)

if __name__ == "__main__":
    main()

